---
title: "C칩mo mejorar predicciones en el Titanic con ingenier칤a de caracter칤sticas"
date: 2025-08-25
---

# C칩mo mejorar predicciones en el Titanic con ingenier칤a de caracter칤sticas

Esta pr치ctica avanza desde el an치lisis exploratorio hacia la construcci칩n de nuestro primer modelo predictivo para el dataset del Titanic.

## Contexto
Despu칠s de explorar los datos en la Pr치ctica 1, el siguiente paso es prepararlos para un algoritmo de Machine Learning. Esto involucra dos procesos clave: la **limpieza de datos** (imputaci칩n de valores faltantes) y el **Feature Engineering** (creaci칩n de nuevas variables predictivas). Finalmente, se construye y eval칰a un modelo simple de Regresi칩n Log칤stica, compar치ndolo contra un modelo base para medir su efectividad.

## Objetivos
- Aplicar t칠cnicas de imputaci칩n de datos para manejar valores faltantes en `Age`, `Fare` y `Embarked`.
- Crear nuevas caracter칤sticas (`FamilySize`, `IsAlone`, `Title`) para mejorar el poder predictivo del dataset.
- Entrenar un modelo `DummyClassifier` como baseline y un modelo `LogisticRegression`.
- Evaluar y comparar el rendimiento de ambos modelos utilizando `accuracy`, `classification_report` y la `matriz de confusi칩n`.

## Actividades (con tiempos estimados)

| Actividad | Tiempo | Resultado Esperado |
| :--- | :---: | :--- |
| **Investigaci칩n de Scikit-learn** | 10 min | Comprensi칩n de los componentes clave. |
| **Preprocesamiento y Features** | 15 min | Dataset limpio y con nuevas columnas. |
| **Entrenamiento de Modelos** | 15 min | Modelos baseline y de Regresi칩n Log칤stica entrenados. |
| **Evaluaci칩n y An치lisis** | 10 min | M칠tricas de rendimiento y conclusiones. |

## Desarrollo
El flujo de trabajo se centr칩 en transformar el dataset crudo en un formato adecuado para el modelado.

1.  **Imputaci칩n de Datos Faltantes:** Se rellenaron los valores nulos con estrategias espec칤ficas para cada columna: la **moda** para `Embarked` (categ칩rica), la **mediana** para `Fare` (num칠rica con outliers), y una **mediana agrupada** por `Sex` y `Pclass` para `Age`, siendo esta 칰ltima una t칠cnica m치s precisa.
2.  **Ingenier칤a de Caracter칤sticas (Feature Engineering):** Se crearon tres nuevas variables a partir de las existentes:
    * `FamilySize`: Unificando `SibSp` y `Parch` para cuantificar el tama침o de la familia.
    * `IsAlone`: Una variable binaria para capturar la condici칩n de viajar solo.
    * `Title`: Extrayendo el t칤tulo del pasajero desde la columna `Name`, lo que result칩 ser un predictor muy potente.
3.  **Preparaci칩n para el Modelo:** Las variables categ칩ricas como `Sex`, `Embarked` y `Title` fueron convertidas a un formato num칠rico mediante One-Hot Encoding (`pd.get_dummies`).
4.  **Modelado y Evaluaci칩n:** El dataset procesado se dividi칩 en conjuntos de entrenamiento (80%) y prueba (20%) de forma estratificada. Se entrenaron y evaluaron un `DummyClassifier` (que predice la clase m치s frecuente) y un modelo de `LogisticRegression`.

## Evidencias
La evidencia principal de esta pr치ctica son los resultados del modelo, que demuestran una mejora significativa sobre el baseline.

???+ info "Resultados del Modelo: Accuracy"

    La `accuracy` (precisi칩n global) muestra una mejora dr치stica del modelo de Regresi칩n Log칤stica sobre el baseline. El modelo entrenado es **20 puntos porcentuales** m치s preciso que una simple suposici칩n basada en la clase m치s frecuente. 游

    ```text
    Baseline acc: 0.6145
    LogReg acc  : 0.8156
    ```

???+ info "Reporte de Clasificaci칩n (Regresi칩n Log칤stica)"

    Este reporte desglosa el rendimiento del modelo para cada clase (0=No sobrevivi칩, 1=Sobrevivi칩), mostrando que el modelo es bastante balanceado, aunque ligeramente mejor prediciendo la clase mayoritaria (0).

    ```text
    Classification report (LogReg):
                  precision    recall  f1-score   support

               0       0.82      0.89      0.86       110
               1       0.80      0.70      0.74        69

        accuracy                           0.82       179
       macro avg       0.81      0.79      0.80       179
    weighted avg       0.81      0.82      0.81       179
    ```

???+ info "Matriz de Confusi칩n (Regresi칩n Log칤stica)"

    La matriz de confusi칩n nos muestra exactamente en qu칠 se equivoca el modelo, siendo los Falsos Negativos (predecir que alguien no sobrevivi칩 cuando s칤 lo hizo) el error m치s com칰n.

    ```text
    [[98 12]
     [21 48]]
    ```
    **Interpretaci칩n de la Matriz:**

    | | Predijo: No sobrevivi칩 | Predijo: Sobrevivi칩 |
    | :--- | :---: | :---: |
    | **Real: No sobrevivi칩** | 98 (VN) | 12 (FP) |
    | **Real: Sobrevivi칩** | 21 (FN) | 48 (VP) |

## Reflexi칩n
- **Qu칠 aprend칤:** Comprend칤 el flujo de trabajo completo para preparar datos y construir un primer modelo. La ingenier칤a de caracter칤sticas no es solo una t칠cnica, es un arte que mejora dr치sticamente el rendimiento. Entend칤 que un modelo simple pero bien evaluado es mucho m치s valioso que uno complejo sin un punto de comparaci칩n.
- **Qu칠 mejorar칤as:** Experimentar칤a con otras variables creadas. Por ejemplo, se podr칤a intentar crear "bins" o categor칤as para la edad (`Ni침o`, `Adulto`, `Mayor`) para ver si el modelo captura mejor esas relaciones.
- **Pr칩ximos Pasos:** El siguiente paso es intentar mejorar el rendimiento del modelo. Esto se puede hacer probando otros algoritmos (como 츼rboles de Decisi칩n o Random Forest) o ajustando los hiperpar치metros del modelo actual de Regresi칩n Log칤stica.

## Referencias
- **Documentaci칩n Scikit-learn:**
    - [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    - [`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)
    - [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
- **Notebook de An치lisis:** [Pr치ctica 2 - Titanic Feature Engineering](https://colab.research.google.com/drive/1e--1F7BANFxsP4bUh_OyEjL2Zd2qSOBB?usp=sharing)
